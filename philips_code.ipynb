{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from maxvolpy.maxvol import rect_maxvol, maxvol\n",
    "import random\n",
    "from itertools import chain, cycle, islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(islice(cycle(range(3)), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does it drop redundant features?\n",
    "\n",
    "$ 2 $ classes. We are generating a sample. Randomly choose one, then generate features for it. Different classes have different distributions of features. We generate $ n $ samples this way, suppose they all have $ k $ features. Replicate each column $ r $ times, so that we have $ n $ samples each with $ rk $ columns now. Add noise to each column.\n",
    "\n",
    "Invoke maxvol to select $ k $ columns. See how many duplicates it gets. The hypothesis we're checking is it selects very few duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(\n",
    "    n, class_1_prob, k, min_feature_std, max_feature_std, how_to_duplicate,\n",
    "    num_duplicates, noise_level, random_seed\n",
    "):\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    # let all features of class 1 have mean 1\n",
    "    # and let all features of class 2 have mean 3\n",
    "    class_1_features_means = 2 * np.ones(k)\n",
    "    class_2_features_means = 4 * np.ones(k)\n",
    "    \n",
    "    # now let's choose standard deviations for features\n",
    "    # for each feature its standard deviation will be the same no matter class 1 or class 2\n",
    "    features_stds = np.linspace(min_feature_std, max_feature_std, num=k)\n",
    "    \n",
    "    # determine how many samples will be of each class\n",
    "    class_1_num_samples = np.random.binomial(n, class_1_prob)\n",
    "    class_2_num_samples = n - class_1_num_samples\n",
    "    \n",
    "    # now let's actually generate the data\n",
    "    class_1_dataset = features_stds * np.random.randn(class_1_num_samples, k) + class_1_features_means\n",
    "    class_2_dataset = features_stds * np.random.randn(class_2_num_samples, k) + class_2_features_means\n",
    "    \n",
    "    # in the following array first class_1_num_samples rows contain objects of class 1\n",
    "    # and all rows after those contain objects of class 2\n",
    "    dataset = np.concatenate((class_1_dataset, class_2_dataset), axis=0)\n",
    "    \n",
    "    return (\n",
    "        duplicate_and_add_noise(\n",
    "            dataset, how_to_duplicate, num_duplicates,\n",
    "            noise_level, min_feature_std, max_feature_std\n",
    "        ),\n",
    "        class_1_num_samples,\n",
    "        class_2_num_samples\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_and_add_noise(\n",
    "    dataset, how_to_duplicate, num_duplicates,\n",
    "    noise_level, min_feature_std, max_feature_std\n",
    "):\n",
    "    k = dataset.shape[1]\n",
    "    dataset = np.tile(dataset, num_duplicates)\n",
    "    \n",
    "    noise_std = noise_level * (min_feature_std + max_feature_std) / 2\n",
    "    dataset += noise_std * np.random.randn(*dataset.shape)\n",
    "    \n",
    "    if how_to_duplicate == \"same\":\n",
    "        return dataset\n",
    "    elif how_to_duplicate == \"transforms\":\n",
    "        # add random shifts\n",
    "        shifts = 10 * dataset.mean(axis=0) * (np.random.rand(k*num_duplicates) - 0.5)\n",
    "        dataset += shifts\n",
    "        \n",
    "        transform_functions = list(islice(\n",
    "            cycle([\n",
    "                lambda arr: arr,\n",
    "                np.exp,\n",
    "                lambda arr: np.sqrt(np.abs(arr)),\n",
    "                lambda arr: np.power(arr, 2),\n",
    "                lambda arr: np.power(arr, 3)\n",
    "            ]),\n",
    "            num_duplicates\n",
    "        ))\n",
    "        for transform, batch in zip(transform_functions, range(num_duplicates)):\n",
    "            dataset[:, batch*k:(batch+1)*k] = transform(dataset[:, batch*k:(batch+1)*k])\n",
    "        return dataset\n",
    "    else:\n",
    "        raise ValueError(\"Incorrect `how_to_duplicate` parameter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 200 # number of true features\n",
    "num_duplicates = 5\n",
    "dataset, class_1_num_samples, class_2_num_samples = build_dataset(\n",
    "    n=10000,\n",
    "    class_1_prob=0.75,\n",
    "    k=k,\n",
    "    min_feature_std=0.5,\n",
    "    max_feature_std=1.5,\n",
    "    how_to_duplicate=\"transforms\",\n",
    "    num_duplicates=num_duplicates,\n",
    "    noise_level=0.2,\n",
    "    random_seed=6666\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[:class_1_num_samples, 0].mean())\n",
    "print(dataset[class_1_num_samples:, 0].mean())\n",
    "\n",
    "# difference should be approximately 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[:class_1_num_samples, k].mean())\n",
    "print(dataset[class_1_num_samples:, k].mean())\n",
    "\n",
    "print(dataset[:class_1_num_samples, k].std())\n",
    "print(dataset[class_1_num_samples:, k].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for each $ i \\in \\{ 0, \\dots, \\text{ num_true_features} \\} $ correlation of each column with number $ i + j \\text{ num_true_features} $ is far from zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "pd.DataFrame(dataset[:, range(i, i + (num_duplicates-1)*k + 1, k)]).corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to choose the best k-column submatrix using `rect_maxvol`.\n",
    "\n",
    "First let's try simply choosing k random rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_dataset = (dataset - dataset.mean(axis=0)) / dataset.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_dataset.mean(axis=0)\n",
    "# all values should be close to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_dataset.std(axis=0)\n",
    "# all values should be close to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_features(dataset, k, samples_choice):\n",
    "    n = dataset.shape[0]\n",
    "    if samples_choice == \"random\":\n",
    "        samples_subset_indices = np.random.choice(n, size=k, replace=False)\n",
    "    elif samples_choice == \"rect_maxvol\":\n",
    "        raise ValueError(\"Not implemented yet\")\n",
    "    else:\n",
    "        raise ValueError(\"Incorrect samples_choice parameter\")\n",
    "    features_subset_indices = rect_maxvol(dataset[samples_subset_indices, :].T, minK=k, maxK=k, tol=0.05)[0]\n",
    "    return features_subset_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_percentage_uniq_features(features_subset_indices):\n",
    "    return len(np.unique(features_subset_indices % k)) / len(features_subset_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_features_indices = choose_features(normalized_dataset, k=k, samples_choice=\"random\")\n",
    "#print(chosen_features_indices)\n",
    "#print(chosen_features_indices % k)\n",
    "print(calculate_percentage_uniq_features(chosen_features_indices)) # more is better"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
