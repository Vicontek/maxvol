{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from maxvolpy.maxvol import rect_maxvol, maxvol\n",
    "import random\n",
    "from itertools import chain, cycle, islice, product\n",
    "import numba\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from functools import lru_cache\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does it drop redundant features?\n",
    "\n",
    "$ 2 $ classes. We are generating a sample. Randomly choose one, then generate features for it. Different classes have different distributions of features. We generate $ n $ samples this way, suppose they all have $ k $ features. Replicate each column $ r $ times, so that we have $ n $ samples each with $ rk $ columns now. Add noise to each column.\n",
    "\n",
    "Invoke maxvol to select $ k $ columns. See how many duplicates it gets. The hypothesis we're checking is it selects very few duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(\n",
    "    n, class_1_prob, k, min_feature_std, max_feature_std, how_to_duplicate,\n",
    "    num_duplicates, noise_level, random_seed=None\n",
    "):\n",
    "    np.random.seed(random_seed)\n",
    "    random.seed(random_seed)\n",
    "    \n",
    "    # let all features of class 1 have mean 1\n",
    "    # and let all features of class 2 have mean 3\n",
    "    class_1_features_means = 2 * np.ones(k)\n",
    "    class_2_features_means = 4 * np.ones(k)\n",
    "    \n",
    "    # now let's choose standard deviations for features\n",
    "    # for each feature its standard deviation will be the same no matter class 1 or class 2\n",
    "    features_stds = np.linspace(min_feature_std, max_feature_std, num=k)\n",
    "    \n",
    "    # determine how many samples will be of each class\n",
    "    class_1_num_samples = np.random.binomial(n, class_1_prob)\n",
    "    class_2_num_samples = n - class_1_num_samples\n",
    "    \n",
    "    # now let's actually generate the data\n",
    "    class_1_dataset = features_stds * np.random.randn(class_1_num_samples, k) + class_1_features_means\n",
    "    class_2_dataset = features_stds * np.random.randn(class_2_num_samples, k) + class_2_features_means\n",
    "        \n",
    "    # in the following array first class_1_num_samples rows contain objects of class 1\n",
    "    # and all rows after those contain objects of class 2\n",
    "    dataset = np.concatenate((class_1_dataset, class_2_dataset), axis=0)\n",
    "    \n",
    "    return (\n",
    "        duplicate_and_add_noise(\n",
    "            dataset, how_to_duplicate, num_duplicates,\n",
    "            noise_level, min_feature_std, max_feature_std\n",
    "        ),\n",
    "        class_1_num_samples,\n",
    "        class_2_num_samples\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_and_add_noise(\n",
    "    dataset, how_to_duplicate, num_duplicates,\n",
    "    noise_level, min_feature_std, max_feature_std\n",
    "):\n",
    "    k = dataset.shape[1]\n",
    "    dataset = np.tile(dataset, num_duplicates)\n",
    "    \n",
    "    noise_std = noise_level * (min_feature_std + max_feature_std) / 2\n",
    "    dataset += noise_std * np.random.randn(*dataset.shape)\n",
    "    \n",
    "    if how_to_duplicate == \"same\":\n",
    "        return dataset\n",
    "    elif how_to_duplicate == \"transforms\":\n",
    "        # add random shifts\n",
    "        shifts = 10 * dataset.mean(axis=0) * (np.random.rand(k*num_duplicates) - 0.5)\n",
    "        dataset += shifts\n",
    "        \n",
    "        transform_functions = list(islice(\n",
    "            cycle([\n",
    "                lambda arr: arr,\n",
    "                np.exp,\n",
    "                lambda arr: np.sqrt(np.abs(arr)),\n",
    "                lambda arr: np.power(arr, 2),\n",
    "                lambda arr: np.power(arr, 3)\n",
    "            ]),\n",
    "            num_duplicates\n",
    "        ))\n",
    "        for transform, batch in zip(transform_functions, range(num_duplicates)):\n",
    "            dataset[:, batch*k:(batch+1)*k] = transform(dataset[:, batch*k:(batch+1)*k])\n",
    "        return dataset\n",
    "    else:\n",
    "        raise ValueError(\"Incorrect `how_to_duplicate` parameter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 200 # number of true features\n",
    "num_duplicates = 5\n",
    "dataset, class_1_num_samples, class_2_num_samples = build_dataset(\n",
    "    n=10000,\n",
    "    class_1_prob=0.75,\n",
    "    k=k,\n",
    "    min_feature_std=0.5,\n",
    "    max_feature_std=1.5,\n",
    "    how_to_duplicate=\"transforms\",\n",
    "    num_duplicates=num_duplicates,\n",
    "    noise_level=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[:class_1_num_samples, 0].mean())\n",
    "print(dataset[class_1_num_samples:, 0].mean())\n",
    "\n",
    "# difference should be approximately 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset[:class_1_num_samples, k].mean())\n",
    "print(dataset[class_1_num_samples:, k].mean())\n",
    "\n",
    "print(dataset[:class_1_num_samples, k].std())\n",
    "print(dataset[class_1_num_samples:, k].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that for each $ i \\in \\{ 0, \\dots, \\text{ num_true_features} \\} $ correlation of each column with number $ i + j \\text{ num_true_features} $ is far from zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "pd.DataFrame(dataset[:, range(i, i + (num_duplicates-1)*k + 1, k)]).corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to choose the best k-column submatrix using `rect_maxvol`.\n",
    "\n",
    "First let's try simply choosing k random rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_dataset = (dataset - dataset.mean(axis=0)) / dataset.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_dataset.mean(axis=0)\n",
    "# all values should be close to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_dataset.std(axis=0)\n",
    "# all values should be close to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_features(dataset, k, samples_choice):\n",
    "    n = dataset.shape[0]\n",
    "    if samples_choice == \"random\":\n",
    "        samples_subset_indices = np.random.choice(n, size=k, replace=False)\n",
    "    elif samples_choice == \"rect_maxvol\":\n",
    "        raise ValueError(\"Not implemented yet\")\n",
    "    else:\n",
    "        raise ValueError(\"Incorrect samples_choice parameter\")\n",
    "    features_subset_indices = rect_maxvol(dataset[samples_subset_indices, :].T, minK=k, maxK=k, tol=0.05)[0]\n",
    "    return features_subset_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_percentage_uniq_features(features_subset_indices, k):\n",
    "    return len(np.unique(features_subset_indices % k)) / len(features_subset_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_features_indices = choose_features(normalized_dataset, k=k, samples_choice=\"random\")\n",
    "#print(chosen_features_indices)\n",
    "#print(chosen_features_indices % k)\n",
    "print(calculate_percentage_uniq_features(chosen_features_indices, k)) # more is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also calculate expected percentage of uniq features if we chose columns randomly.\n",
    "\n",
    "We have $ k $ true features, each replicated $ r $ times.\n",
    "\n",
    "Math is difficult, so I'll calculate this by experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=1000)\n",
    "@numba.jit(nopython=True)\n",
    "def calculate_expected_uniq_features(k, num_duplicates):\n",
    "    total_count = 0\n",
    "    num_experiments = 10000\n",
    "    for i in range(num_experiments):\n",
    "        chosen_true_features = np.random.choice(k*num_duplicates, k, replace=False) % k\n",
    "        num_distinct_features = len(np.unique(chosen_true_features))\n",
    "        total_count += num_distinct_features\n",
    "    return total_count / (k * num_experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_expected_uniq_features(k, num_duplicates)\n",
    "\n",
    "# our hypothesis is that this number will be significantly lower than the number when\n",
    "# we choose features using maxvol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot a plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_experiments(duplication_method, k_values, min_feature_stds, num_duplicates, class_1_prob=0.75):\n",
    "    assert len(k_values) == len(min_feature_stds)\n",
    "    for (k, min_feature_std) in tqdm(list(zip(k_values, min_feature_stds))):\n",
    "        num_experiments = 10\n",
    "        for i in range(num_experiments):\n",
    "            dataset, class_1_num_samples, class_2_num_samples = build_dataset(\n",
    "                n=10000,\n",
    "                class_1_prob=class_1_prob,\n",
    "                k=k,\n",
    "                min_feature_std=min_feature_std,\n",
    "                max_feature_std=min_feature_std+1,\n",
    "                how_to_duplicate=duplication_method,\n",
    "                num_duplicates=num_duplicates,\n",
    "                noise_level=0.1,\n",
    "            )\n",
    "            normalized_dataset = (dataset - dataset.mean(axis=0)) / dataset.std(axis=0)\n",
    "            chosen_features_indices = choose_features(normalized_dataset, k=k, samples_choice=\"random\")\n",
    "            experiments_data.append({\n",
    "                \"k\": k,\n",
    "                \"std\": min_feature_std+0.5,\n",
    "                \"duplication_method\": duplication_method,\n",
    "                \"maxvol feature selection D(I)\": calculate_percentage_uniq_features(chosen_features_indices, k),\n",
    "                \"random feature selection D(I)\": calculate_expected_uniq_features(k, num_duplicates),\n",
    "                \"class_1_prob\": class_1_prob\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_data = []\n",
    "k=60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_experiments(\"same\", [k]*10, min_feature_stds=np.linspace(0.1, 6, num=10), num_duplicates=num_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_experiments(\"transforms\", [k]*10, min_feature_stds=np.linspace(0.1, 6, num=10), num_duplicates=num_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [int(4 * 100**(i/19)) for i in range(17)]\n",
    "# Now do experiment for changing k\n",
    "do_experiments(\"same\", k_values, min_feature_stds=[3] * 17, num_duplicates=num_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_experiments(\"transforms\", k_values, min_feature_stds=[3] * 17, num_duplicates=num_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(experiments_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_same = df[(df.duplication_method == \"same\") & (df.k == k) & (df[\"class_1_prob\"] == 0.75)] \\\n",
    "    [[\"std\", \"maxvol feature selection D(I)\", \"random feature selection D(I)\"]] \\\n",
    "    \\\n",
    "    .groupby(\"std\")\n",
    "fig, ax = plt.subplots()\n",
    "gp_same.mean().plot(yerr=2*gp_same.std(), ax=ax)\n",
    "ax.set_ylim(-0.05, 1.05)\n",
    "ax.set_ylabel(\"percentage of distinct features D(I)\")\n",
    "ax.set_xlabel('standard deviation of \"true features\", $ (\\sigma_{min} + \\sigma_{max})/2 $', usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_transforms = df[(df.duplication_method == \"transforms\") & (df.k == k) & (df[\"class_1_prob\"] == 0.75)] \\\n",
    "    [[\"std\", \"maxvol feature selection D(I)\", \"random feature selection D(I)\"]] \\\n",
    "    .groupby(\"std\")\n",
    "fig, ax = plt.subplots()\n",
    "gp_transforms.mean().plot(yerr=2*gp_same.std(), ax=ax)\n",
    "ax.set_ylim(-0.05, 1.05)\n",
    "ax.set_ylabel(\"percentage of distinct features D(I)\")\n",
    "ax.set_xlabel('standard deviation of \"true features\", $ (\\sigma_{min} + \\sigma_{max})/2 $', usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot for changing k\n",
    "gp_k_same = df[(df.duplication_method == \"same\") & (df[\"std\"] == 3.5) & (df[\"class_1_prob\"] == 0.75)] \\\n",
    "    [[\"k\", \"maxvol feature selection D(I)\", \"random feature selection D(I)\"]] \\\n",
    "    .groupby(\"k\")\n",
    "fig, ax = plt.subplots()\n",
    "gp_k_same.mean().plot(yerr=2*gp_k_same.std(), ax=ax)\n",
    "ax.set_ylim(-0.05, 1.05)\n",
    "ax.set_ylabel(\"percentage of distinct features, D(I)\")\n",
    "ax.set_xlabel(\"number of true features, k\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot for changing k\n",
    "gp_k_transforms = df[(df.duplication_method == \"transforms\") & (df[\"std\"] == 3.5) & (df[\"class_1_prob\"] == 0.75)] \\\n",
    "    [[\"k\", \"maxvol feature selection D(I)\", \"random feature selection D(I)\"]] \\\n",
    "    .groupby(\"k\")\n",
    "fig, ax = plt.subplots()\n",
    "gp_k_transforms.mean().plot(yerr=2*gp_k_same.std(), ax=ax)\n",
    "ax.set_ylim(-0.05, 1.05)\n",
    "ax.set_ylabel(\"percentage of distinct features, D(I)\")\n",
    "ax.set_xlabel(\"number of true features, k\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two plots seem weird to me, so let's check what will happen if we only have one class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_experiments(\"same\", [k]*10, min_feature_stds=np.linspace(0.1, 6, num=10), num_duplicates=num_duplicates, class_1_prob=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_experiments(\"transforms\", [k]*10, min_feature_stds=np.linspace(0.1, 6, num=10), num_duplicates=num_duplicates, class_1_prob=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(experiments_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_same = df[(df.duplication_method == \"same\") & (df[\"class_1_prob\"] == 1)][[\"std\", \"maxvol feature selection D(I)\", \"random feature selection D(I)\"]] \\\n",
    "    .groupby(\"std\")\n",
    "fig, ax = plt.subplots()\n",
    "gp_same.mean().plot(yerr=2*gp_same.std(), ax=ax)\n",
    "ax.set_ylim(-0.05, 1.05)\n",
    "ax.set_ylabel(\"percentage of distinct features, D(I)\")\n",
    "ax.set_xlabel('standard deviation of \"true features\", $ (\\sigma_{min} + \\sigma_{max})/2 $', usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_same = df[(df.duplication_method == \"transforms\") & (df[\"class_1_prob\"] == 1)][[\"std\", \"maxvol feature selection D(I)\", \"random feature selection D(I)\"]] \\\n",
    "    .groupby(\"std\")\n",
    "fig, ax = plt.subplots()\n",
    "gp_same.mean().plot(yerr=2*gp_same.std(), ax=ax)\n",
    "ax.set_ylim(-0.05, 1.05)\n",
    "ax.set_ylabel(\"percentage of distinct features, D(I)\")\n",
    "ax.set_xlabel('standard deviation of \"true features\", $ (\\sigma_{min} + \\sigma_{max})/2 $', usetex=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
